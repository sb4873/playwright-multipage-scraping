{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping basics for Playwright\n",
    "\n",
    "If you feel comfortable with scraping in general, you're free to skip this notebook and try to go right to the next one. Same thing if you get bored partway down.\n",
    "\n",
    "> The [scraping section](https://jonathansoma.com/everything/scraping/) on my Everything I Know site might be helpful.\n",
    ">\n",
    "> I know I love them, but **you don't have to use CSS selectors!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Imports\n",
    "\n",
    "Import what you need to use Playwright, and start up a new browser to use for scraping. \n",
    "\n",
    "> If you end up opening a lot of Chromes/Chromiums, shutting down the Python kernel with the stop button is an easy way to make them go away! You'll have to re-run your notebook, but at least you won't have sixty icons in your dock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "\n",
    "# Create a new browser window\n",
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Scraping by class\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/by-class.html, printing out the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/lede/static/by-class.html' request=<Request url='https://jonathansoma.com/lede/static/by-class.html' method='GET'>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/by-class.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><h1 class=\"title\">How to Scrape Things</h1>\n",
       "<h3 class=\"subhead\">Some Supplemental Materials</h3>\n",
       "<p class=\"byline\">By Jonathan Soma</p></body></html>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "soup_doc = BeautifulSoup(html)\n",
    "soup_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Scrape Things\n",
      "Some Supplemental Materials\n",
      "By Jonathan Soma\n"
     ]
    }
   ],
   "source": [
    "# I am going to use CSS selectors exclusively here to see how I feel about them\n",
    "\n",
    "title = soup_doc.select_one('.title')\n",
    "subhead = soup_doc.select_one('.subhead')\n",
    "byline = soup_doc.select_one('.byline')\n",
    "\n",
    "print(title.text)\n",
    "print(subhead.text)\n",
    "print(byline.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scraping using tags\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/by-tag.html, printing out the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/lede/static/by-tag.html' request=<Request url='https://jonathansoma.com/lede/static/by-tag.html' method='GET'>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/by-tag.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><h1>How to Scrape Things</h1>\n",
       "<h3>Some Supplemental Materials</h3>\n",
       "<p>By Jonathan Soma</p></body></html>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "soup_doc = BeautifulSoup(html)\n",
    "soup_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Scrape Things\n",
      "Some Supplemental Materials\n",
      "By Jonathan Soma\n"
     ]
    }
   ],
   "source": [
    "title = soup_doc.select_one('h1')\n",
    "subhead = soup_doc.select_one('h3')\n",
    "byline = soup_doc.select_one('p')\n",
    "\n",
    "print(title.text)\n",
    "print(subhead.text)\n",
    "print(byline.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scraping using a single tag\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/by-list.html, creating a dictionary out of the title, subhead, and byline in sentences, e.g. \"the title is `______`\"\n",
    "\n",
    "> **This will be important for the next few:** you can use `.get_by_text` but it seems kind of silly since maybe the text would change. I think getting them all, then using list indexes like `[0]`, etc, would be better! If I sold you on CSS selectors, you can also look up `nth-of-type` and use it with `.select_one`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/lede/static/by-list.html' request=<Request url='https://jonathansoma.com/lede/static/by-list.html' method='GET'>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/by-list.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><p>How to Scrape Things</p>\n",
       "<p>Some Supplemental Materials</p>\n",
       "<p>By Jonathan Soma</p></body></html>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "soup_doc = BeautifulSoup(html)\n",
    "soup_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note to self:\n",
    "The :nth-of-type(n) selector matches every element that is the nth child, of the same type (tag name), of its parent.\n",
    "\n",
    "n can be a number, a keyword (odd or even), or a formula (like an + b).\n",
    "\n",
    "Syntax:\n",
    "\n",
    "```\n",
    ":nth-of-type(number) {\n",
    "  css declarations;\n",
    "}\n",
    "```\n",
    "\n",
    "or more relevant here: Select the third \"td\" element in a row\n",
    "```\n",
    "td:nth-child(3)\n",
    "```\n",
    "Note: the first element is nth-child(1), not nth-child(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The title is How to Scrape Things',\n",
       " 'subhead': 'The subhead is Some Supplemental Materials',\n",
       " 'byline': 'The byline is By Jonathan Soma'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1={}\n",
    "\n",
    "dict1['title'] = f\"The title is {soup_doc.select_one('p:nth-child(1)').text}\"\n",
    "dict1['subhead'] = f\"The subhead is {soup_doc.select_one('p:nth-child(2)').text}\"\n",
    "dict1['byline'] = f\"The byline is {soup_doc.select_one('p:nth-child(3)').text}\"\n",
    "\n",
    "dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Scraping a single table row\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/single-table-row.html, printing out the title, subhead, and byline in sentences, e.g. \"the title is `______`.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/lede/static/single-table-row.html' request=<Request url='https://jonathansoma.com/lede/static/single-table-row.html' method='GET'>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/single-table-row.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><table>\n",
       "<tbody><tr>\n",
       "<td>How to Scrape Things</td>\n",
       "<td>Some Supplemental Materials</td>\n",
       "<td>By Jonathan Soma</td>\n",
       "</tr>\n",
       "</tbody></table></body></html>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "soup_doc = BeautifulSoup(html)\n",
    "soup_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title is How to Scrape Things\n",
      "The subhead is Some Supplemental Materials\n",
      "The byline is By Jonathan Soma\n"
     ]
    }
   ],
   "source": [
    "title = f\"The title is {soup_doc.select_one('td:nth-child(1)').text}\"\n",
    "subhead = f\"The subhead is {soup_doc.select_one('td:nth-child(2)').text}\"\n",
    "byline = f\"The byline is {soup_doc.select_one('td:nth-child(3)').text}\"\n",
    "\n",
    "print(title)\n",
    "print(subhead)\n",
    "print(byline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Saving into a dictionary\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/single-table-row.html, saving the title, subhead, and byline into a single dictionary called `book`.\n",
    "\n",
    "> Don't use pandas for this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Some Supplemental Materials',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book={}\n",
    "\n",
    "book['title'] = soup_doc.select_one('td:nth-child(1)').text\n",
    "book['subhead'] = soup_doc.select_one('td:nth-child(2)').text\n",
    "book['byline'] = soup_doc.select_one('td:nth-child(3)').text\n",
    "\n",
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Scraping multiple table rows\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/multiple-table-rows.html, printing out each title, subhead, and byline.\n",
    "\n",
    "> You won't use pandas for this one, either!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/lede/static/multiple-table-rows.html' request=<Request url='https://jonathansoma.com/lede/static/multiple-table-rows.html' method='GET'>>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/multiple-table-rows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><table>\n",
       "<tbody><tr>\n",
       "<td>How to Scrape Things</td>\n",
       "<td>Some Supplemental Materials</td>\n",
       "<td>By Jonathan Soma</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>How to Scrape Many Things</td>\n",
       "<td>But, Is It Even Possible?</td>\n",
       "<td>By Sonathan Joma</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>The End of Scraping</td>\n",
       "<td>Let's All Use CSV Files</td>\n",
       "<td>By Amos Nathanos</td>\n",
       "</tr>\n",
       "</tbody></table></body></html>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "soup_doc = BeautifulSoup(html)\n",
    "soup_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'How to Scrape Things',\n",
       "  'Subhead': 'Some Supplemental Materials',\n",
       "  'Byline': 'By Jonathan Soma'},\n",
       " {'Title': 'How to Scrape Many Things',\n",
       "  'Subhead': 'But, Is It Even Possible?',\n",
       "  'Byline': 'By Sonathan Joma'},\n",
       " {'Title': 'The End of Scraping',\n",
       "  'Subhead': \"Let's All Use CSV Files\",\n",
       "  'Byline': 'By Amos Nathanos'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rows = soup_doc.select('tr')\n",
    "\n",
    "rows = []\n",
    "for row in all_rows :\n",
    "    title = row.select_one('td:nth-child(1)').text\n",
    "    subhead = row.select_one('td:nth-child(2)').text\n",
    "    byline = row.select_one('td:nth-child(3)').text\n",
    "    one_row = [title, subhead, byline]\n",
    "    rows.append(one_row)\n",
    "rows\n",
    "\n",
    "keys = ['Title', 'Subhead', 'Byline']\n",
    "\n",
    "to_dict = []\n",
    "for each in rows:\n",
    "    this_row = dict(zip(keys,each))\n",
    "    to_dict.append(this_row)\n",
    "to_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Scraping an actual table\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/the-actual-table.html, creating a list of dictionaries.\n",
    "\n",
    "> Don't use pandas here, either, even though that's exactly what we did in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/lede/static/the-actual-table.html' request=<Request url='https://jonathansoma.com/lede/static/the-actual-table.html' method='GET'>>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/the-actual-table.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><table id=\"booklist\">\n",
       "<tbody><tr>\n",
       "<td>How to Scrape Things</td>\n",
       "<td>Some Supplemental Materials</td>\n",
       "<td>By Jonathan Soma</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>How to Scrape Many Things</td>\n",
       "<td>But, Is It Even Possible?</td>\n",
       "<td>By Sonathan Joma</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>The End of Scraping</td>\n",
       "<td>Let's All Use CSV Files</td>\n",
       "<td>By Amos Nathanos</td>\n",
       "</tr>\n",
       "</tbody></table></body></html>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "soup_doc = BeautifulSoup(html)\n",
    "soup_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'How to Scrape Things',\n",
       "  'Subhead': 'Some Supplemental Materials',\n",
       "  'Byline': 'By Jonathan Soma'},\n",
       " {'Title': 'How to Scrape Many Things',\n",
       "  'Subhead': 'But, Is It Even Possible?',\n",
       "  'Byline': 'By Sonathan Joma'},\n",
       " {'Title': 'The End of Scraping',\n",
       "  'Subhead': \"Let's All Use CSV Files\",\n",
       "  'Byline': 'By Amos Nathanos'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I did that for the previous question for self-learning but it made this a lot easier\n",
    "\n",
    "all_rows = soup_doc.select('tr')\n",
    "\n",
    "rows = []\n",
    "for row in all_rows :\n",
    "    title = row.select_one('td:nth-child(1)').text\n",
    "    subhead = row.select_one('td:nth-child(2)').text\n",
    "    byline = row.select_one('td:nth-child(3)').text\n",
    "    one_row = [title, subhead, byline]\n",
    "    rows.append(one_row)\n",
    "rows\n",
    "\n",
    "keys = ['Title', 'Subhead', 'Byline']\n",
    "\n",
    "to_dict = []\n",
    "for each in rows:\n",
    "    this_row = dict(zip(keys,each))\n",
    "    to_dict.append(this_row)\n",
    "to_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Scraping multiple table rows into a list of dictionaries\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/the-actual-table.html, creating a pandas DataFrame.\n",
    "\n",
    "> There are two ways to do this one! One uses just pandas, the other one uses the result from Part 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subhead</th>\n",
       "      <th>Byline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Scrape Things</td>\n",
       "      <td>Some Supplemental Materials</td>\n",
       "      <td>By Jonathan Soma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Scrape Many Things</td>\n",
       "      <td>But, Is It Even Possible?</td>\n",
       "      <td>By Sonathan Joma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The End of Scraping</td>\n",
       "      <td>Let's All Use CSV Files</td>\n",
       "      <td>By Amos Nathanos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Title                      Subhead            Byline\n",
       "0       How to Scrape Things  Some Supplemental Materials  By Jonathan Soma\n",
       "1  How to Scrape Many Things    But, Is It Even Possible?  By Sonathan Joma\n",
       "2        The End of Scraping      Let's All Use CSV Files  By Amos Nathanos"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(to_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Scraping into a file\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/the-actual-table.html and save it as `output.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
